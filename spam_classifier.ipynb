{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from helperfunctions import get_enron_raw_dataset, get_lingspam_dataset\n",
    "import re\n",
    "import html\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "# html reader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists\n",
      "file exists\n"
     ]
    }
   ],
   "source": [
    "# Download the Enron Raw enron_emails and parse enron dataset\n",
    "enron_emails = get_enron_raw_dataset()\n",
    "lingspam_df = get_lingspam_dataset()\n",
    "\n",
    "# merge datasets\n",
    "# emails = enron_emails.append(lingspam_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Cleaning Natural Language Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "\n",
    "# check if the email is empty\n",
    "def check_nan(x):\n",
    "    if str(x) == \"nan\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "    \n",
    "# header and tcp package information and \n",
    "# just keep the subject and the body of the email\n",
    "def isolate_subject_body(x):\n",
    "    clean_message = []\n",
    "    in_message = False\n",
    "    subject_included = False\n",
    "    for line in x.split(\"\\n\"):\n",
    "        if \"subject:\" in line.lower() and subject_included == False:\n",
    "            clean_message.append(line)\n",
    "            subject_included = True\n",
    "            \n",
    "        if line == \"\":\n",
    "            in_message = True\n",
    "        if in_message:\n",
    "            clean_message.append(line)\n",
    "    return \"\\n\".join(clean_message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parse all html tags from \n",
    "# text\n",
    "def get_text_bs(html):\n",
    "    tree = BeautifulSoup(html, 'lxml')\n",
    "    body = tree.body\n",
    "    for tag in body.select('script'):\n",
    "        tag.decompose()\n",
    "    for tag in body.select('style'):\n",
    "        tag.decompose()          \n",
    "    text = body.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def remove_html_imperfections(x):\n",
    "    x = x.replace(\"&amp;\", \"&\").replace(\"=\\n\", \"\").replace(\"=20\", \"\").replace(\"3D\", \"\").replace(\"=B7\", \"\").replace(\"=09\", \"\").replace(\"=0D\", \"\")\n",
    "    x = html.unescape(x)\n",
    "    x = x.lower()\n",
    "    p = re.compile(r'(href).+\"|href.+(\\w.org|\\w.[.]com|\\/[a-z0-9]*\\/|\\w[.]html)|[a-z]*=[a-z0-9]*<br>|[a-z0-9]*=\"[a-z0-9]*\"<br>*>|<[^a-z]*<br>')\n",
    "    x = p.sub('', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remove all html tags and leave only\n",
    "# visible text\n",
    "def parse_html(x):\n",
    "    try:\n",
    "        x = remove_html_imperfections(x)\n",
    "        if \"<\" in x and \"</\" in x and \">\" in x:\n",
    "            return get_text_bs(x)\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        print(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# remove subject word from\n",
    "# email body\n",
    "def remove_subject(x):\n",
    "    try:\n",
    "        if \"subject\" in x:\n",
    "            x = x.replace(\"subject\", \"\")\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        print(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# clean emails\n",
    "# remove subject word\n",
    "# remove line breaks\n",
    "# remove non alphanumeric characters\n",
    "# remove numbers\n",
    "# \n",
    "    \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "porter = nltk.PorterStemmer()\n",
    "def preprocess_text(messy_string):\n",
    "    try:\n",
    "        cleaned = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', messy_string)\n",
    "        cleaned = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', cleaned)\n",
    "        cleaned = re.sub(r'Â£|\\$', 'moneysymb', cleaned)\n",
    "        cleaned = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b','phonenumbr', cleaned)\n",
    "        cleaned = re.sub(r'\\d+(\\.\\d+)?', 'numbr', cleaned)\n",
    "        cleaned = re.sub(r'[^\\w\\d\\s]', ' ', cleaned)\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        cleaned = re.sub(r'^\\s+|\\s+?$', '', cleaned.lower())\n",
    "        return ' '.join(\n",
    "            porter.stem(term) \n",
    "            for term in cleaned.split()\n",
    "            if term not in set(stop_words)\n",
    "        )\n",
    "    except:\n",
    "        print(messy_string)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Get clean enron\n",
    "# or download and get current version of enron\n",
    "def get_clean_enron(enron_emails=None):\n",
    "    filename = \"{}/enron_raw/enron_raw_preprocessed.csv\".format(path)\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    if file_exists:\n",
    "        enron_emails = pd.read_csv(filename)\n",
    "    elif not file_exists and enron_emails != None:\n",
    "        # check emails that have nan in the email\n",
    "        contain_nan = enron_emails[\"emails\"].apply(check_nan)\n",
    "        enron_emails = enron_emails.loc[contain_nan]\n",
    "        #get only subject and body \n",
    "        enron_emails[\"emails\"] = enron_emails[\"emails\"].apply(isolate_subject_body)\n",
    "        enron_emails[\"emails\"] = enron_emails[\"emails\"].apply(parse_html)\n",
    "        enron_emails.to_csv(filename, index=False)\n",
    "    return enron_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_emails = get_clean_enron(enron_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the enron emails dont have html anymore\n",
    "# enron_emails\n",
    "emails = enron_emails.append(lingspam_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "emails[\"emails\"] = emails[\"emails\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"ham\",\n",
    "    \"spam\"\n",
    "]\n",
    "\n",
    "# 1 = spam, 0 = ham\n",
    "emails[\"types\"] = emails[\"types\"].apply(lambda x: labels.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# remove the subject word\n",
    "\n",
    "emails[\"emails\"] = emails[\"emails\"].apply(remove_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = emails.loc[emails[\"emails\"].apply(lambda x: True if x != None else False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = emails[\"emails\"]\n",
    "y = emails[\"types\"]\n",
    "\n",
    "# Use TfIDF\n",
    "# tfidf_transformer = vectorizer.fit(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "  ('tfidf', TfidfTransformer()),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789909015715468"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829017636763452"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = \"spamClassifier.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(clf, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:  \n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
